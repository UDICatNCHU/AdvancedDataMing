{"cells":[{"cell_type":"code","source":["display(dbutils.fs.ls(\"/FileStore/tables\"))"],"metadata":{"collapsed":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":["weather_data = sc.textFile(\"/FileStore/tables/pm2_5Taiwan-3b69f.csv\")"],"metadata":{"collapsed":false},"outputs":[],"execution_count":2},{"cell_type":"code","source":["weather_data_rdd = weather_data.map(lambda line : line.split(\",\"))\nfor x in weather_data_rdd.take(30):\n    for i in range(len(x)):\n        print x[i],\n    print \"\""],"metadata":{"collapsed":false},"outputs":[],"execution_count":3},{"cell_type":"code","source":["pm25schema = weather_data_rdd.first()\nfor i in pm25schema:\n  print i,"],"metadata":{"collapsed":false},"outputs":[],"execution_count":4},{"cell_type":"code","source":["import math\ndef remove_row_with_noise (x):\n    for i in range(3, len(x)):\n        if not x[i].isdecimal():\n            return False\n    return True "],"metadata":{"collapsed":false},"outputs":[],"execution_count":5},{"cell_type":"code","source":["clean_weather_data = weather_data_rdd\\\n                    .filter(lambda x: x!=pm25schema)\\\n                    .filter(remove_row_with_noise)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["# 練習1: 讓我們求取2015年，大里每小時的平均pm25數值。"],"metadata":{}},{"cell_type":"markdown","source":["### 把大里站之PM25資料撈出來"],"metadata":{}},{"cell_type":"code","source":["dalipm25 = clean_weather_data.filter(lambda x: x[1] == u'大里' and x[2]== \"PM2.5\")\nlis = dalipm25.take(10)\nfor x in lis:\n    for i in range(len(x)):\n        print x[i],\n    print "],"metadata":{"collapsed":false},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["# 作法：將每小時資料轉成(小時,pm數值)，以求取每小時的平均值。\n\n例如：\n    2015/01/01 大里 PM2.5 53 55 58 53 43 36 35 42 55 64 65 59 52 44 47 41 43 40 42 35 28 20 18 16\n    --> [('hr_0', 53.0) ('hr_1', 55.0) ('hr_2', 58.0) ('hr_3', 53.0) ('hr_4', 43.0) ('hr_5', 36.0) ('hr_6', 35.0) ('hr_7', 42.0) ('hr_8', 55.0) ('hr_9', 64.0) ('hr_10', 65.0) ('hr_11', 59.0) ('hr_12', 52.0) ('hr_13', 44.0) ('hr_14', 47.0) ('hr_15', 41.0) ('hr_16', 43.0) ('hr_17', 40.0) ('hr_18', 42.0) ('hr_19', 35.0) ('hr_20', 28.0) ('hr_21', 20.0) ('hr_22', 18.0) ('hr_23', 16.0)]"],"metadata":{}},{"cell_type":"code","source":["def hourKeyGen(x):\n    hourkeypair = []\n    for i in range(3,27):\n        hourkeypair.append((\"hr_\"+str(i-3),float(x[i])))\n    return hourkeypair\n\ncount = dalipm25.count()\nHourSum = dalipm25\\\n            .flatMap(hourKeyGen)\\\n            .reduceByKey(lambda x,y: x+y)\\\n            .mapValues(lambda x: x/count)\\\n            .map(lambda x: (x[1],x[0])).top(24)\n\nprint HourSum"],"metadata":{"collapsed":false},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["# 使用DataFrame 來計算每小時平均值"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import SQLContext\nfrom pyspark.sql import Row\ndalipm25row = dalipm25.map(lambda p:\n        Row(\n        date = p[0],\n        location = p[1],\n        measure = p[2],\n        hr_01 = float(p[3]), hr_02 = float(p[4]),hr_03 = float(p[5]),hr_04 = float(p[6]),hr_05 = float(p[7]),\n        hr_06 = float(p[8]),hr_07 = float(p[9]),hr_08 = float(p[10]),hr_09 = float(p[11]),hr_10 = float(p[12]),\n        hr_11 = float(p[13]),hr_12 = float(p[14]),hr_13 = float(p[15]),hr_14 = float(p[16]),hr_15 = float(p[17]),\n        hr_16 = float(p[18]),hr_17 = float(p[19]),hr_18 = float(p[20]),hr_19 = float(p[21]),hr_20 = float(p[22]),\n        hr_21 = float(p[23]),hr_22 = float(p[24]),hr_23 = float(p[25]),hr_24 = float(p[26]),\n    )\n)\n\n\n"],"metadata":{"collapsed":false},"outputs":[],"execution_count":13},{"cell_type":"code","source":["df = sqlContext.createDataFrame(dalipm25row)\ndf.show()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":14},{"cell_type":"code","source":["df.count()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":15},{"cell_type":"code","source":["df.printSchema()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["## 使用 DataFrame.agg() 來進行column數值之統計計算 http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame"],"metadata":{}},{"cell_type":"code","source":["df.filter(df.hr_01>30).show()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["## 使用 DataFrame.filter() 來進行row資料之條件計算 http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame"],"metadata":{}},{"cell_type":"code","source":["df.filter(df.hr_01>50).select(\"hr_01\",\"location\",\"measure\").show()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["## 使用 DataFrame.select() 來進行資料之Projection http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import functions as F\ndf.agg(F.mean(df.hr_01)).show()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["## 使用 DataFrame.describe() 來進行DataFrame or Column 資料之統計 http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame"],"metadata":{}},{"cell_type":"code","source":["df.describe().show()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["# 使用 Spark SQL來下達SQL查詢"],"metadata":{}},{"cell_type":"code","source":["df.registerTempTable(\"DaliTable\")"],"metadata":{"collapsed":true},"outputs":[],"execution_count":26},{"cell_type":"code","source":["sqlContext.sql(\"\"\"\n                select * from DaliTable where date ='2015/02/07'\n               \"\"\").show()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":27},{"cell_type":"code","source":["sqlContext.sql(\"\"\"\n                select count(*) count \n                from DaliTable \n                where hr_01 > 100\n               \"\"\").show()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":28},{"cell_type":"code","source":["sqlContext.sql(\"\"\"\n                select AVG(hr_01) count \n                from DaliTable\n               \"\"\").show()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":29},{"cell_type":"code","source":["sqlContext.sql(\"\"\"\n                select date,location, hr_01, hr_02, hr_01-hr_02 as diff \n                from DaliTable\n                \"\"\").show()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":30},{"cell_type":"code","source":["sqlContext.sql(\"\"\"\n                select date,location, hr_01, hr_02, hr_01-hr_02 as diff \n                from DaliTable \n                order by diff DESC\n                \"\"\").show()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":31},{"cell_type":"code","source":["sqlContext.sql(\"\"\"\n                select date,location, hr_01+hr_02+hr_03+hr_04+hr_05+hr_06+hr_07+hr_08 as sum \n                from DaliTable \n                order by sum DESC\n                \"\"\").show()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":32},{"cell_type":"code","source":["sqlContext.sql(\"\"\"\n                select first(location) as location, first(date), avg(hr_01) as hr_01, avg(hr_02) as hr_02, avg(hr_03) as hr_03, avg(hr_04) as hr_04,\n                    avg(hr_05) as hr_05, avg(hr_06) as hr_06, avg(hr_07) as hr_07, avg(hr_08) as hr_08\n                from DaliTable\n            \"\"\").show()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":33},{"cell_type":"code","source":[""],"metadata":{"collapsed":false},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["# Perform Pearson Correlation using DataFrame.corr( )\n# 練習: 計算大里區pm10, pm2.5 間之關聯度 http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n使用前面所建立的clean_weather_data rdd資料\n    \n    \n    corr(col1, col2, method=None)\n    Calculates the correlation of two columns of a DataFrame as a double value. Currently only supports the Pearson Correlation Coefficient. DataFrame.corr() and DataFrameStatFunctions.corr() are aliases of each other.\n\n    Parameters:\t\n    col1 – The name of the first column\n    col2 – The name of the second column\n    method – The correlation method. Currently only supports “pearson”\n    New in version 1.4."],"metadata":{}},{"cell_type":"code","source":["def Generated_Measurement(x):\n    date = x[0]\n    location = x[1]\n    measure = x[2]\n    measurements_of_a_day = []\n    for i in range(3, len(x)):\n        measurements_of_a_day.append((date, measure, \"hr\"+str(i-3), x[i]))\n    return measurements_of_a_day\n\ndaliData = clean_weather_data.filter(lambda x: x[1]==u\"大里\" and (x[2] == u\"PM2.5\" or x[2] == u\"PM10\" ))\ndaliData.cache()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":36},{"cell_type":"code","source":["from pyspark.sql import Row\n\ndaliDataRow = \\\n     daliData\\\n    .flatMap(Generated_Measurement)\\\n    .map(lambda x: ( (x[0], x[2]), x[1], x[3] ) )\\\n    .groupBy(lambda x: x[0])\\\n    .filter(lambda x: len(x[1])==2)\\\n    .mapValues(lambda x: list(x))\\\n    .mapValues(lambda x: [x[0][1], x[0][2], x[1][1], x[1][2]])\\\n    .map(lambda x:[ x[0][0], x[0][1], x[1][1], x[1][3]])\\\n    .map(lambda x: Row(\n            date = x[0],\n            time = x[1],\n            pm10 = float(x[2]),\n            pm25 = float(x[3])\n        ))\n    \ndf = sqlContext.createDataFrame(daliDataRow)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":37},{"cell_type":"code","source":["df.show(10)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":38},{"cell_type":"code","source":["from pyspark.sql import functions as F\ndf.agg(F.avg(df.pm10)).show()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":39},{"cell_type":"code","source":["df.corr(\"pm10\",\"pm25\")"],"metadata":{"collapsed":false},"outputs":[],"execution_count":40},{"cell_type":"code","source":["df.describe().show()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":["# 作業1 (個人): 請使用SPARK SQL求取2015年，全國pm2.5最高的前十個工作站測點以及其日期。"],"metadata":{}},{"cell_type":"markdown","source":["# 作業2 (團體): 請求取2015年，全國與大里區pm2.5濃度關聯度最高的前三個工作站測點。"],"metadata":{}},{"cell_type":"markdown","source":["# 作業3(個人): 請算算看2015全國哪個測站，紫爆天數最多？"],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{"collapsed":true},"outputs":[],"execution_count":45}],"metadata":{"kernelspec":{"display_name":"Python 2 with Spark 2.0","language":"python","name":"python2-spark20"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython2","codemirror_mode":{"name":"ipython","version":2},"version":"2.7.11","nbconvert_exporter":"python","file_extension":".py"},"name":"DataFrame Tutorial","notebookId":1414086157091502},"nbformat":4,"nbformat_minor":0}
